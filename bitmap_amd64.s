// Code generated by command: go run asm.go -out ../bitmap_amd64.s -stubs ../stub_amd64.go -pkg=bitmap. DO NOT EDIT.

// +build !appengine
// +build !noasm
// +build gc

#include "textflag.h"

// func x64and(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT 路x64and(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ a_len+8(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "AND" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPAND   (AX), Y0, Y0
	VPAND   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "AND" operation
	MOVQ (CX), BX
	ANDQ (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64andn(a []uint64, b []uint64)
// Requires: AVX, AVX2, BMI
TEXT 路x64andn(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ a_len+8(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "ANDNOT" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPANDN  (AX), Y0, Y0
	VPANDN  32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "ANDNOT" operation
	MOVQ  (CX), BX
	ANDNQ (AX), BX, BX
	MOVQ  BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64or(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT 路x64or(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ a_len+8(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "OR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPOR    (AX), Y0, Y0
	VPOR    32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "OR" operation
	MOVQ (CX), BX
	ORQ  (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET

// func x64xor(a []uint64, b []uint64)
// Requires: AVX, AVX2
TEXT 路x64xor(SB), NOSPLIT, $0-48
	MOVQ a_base+0(FP), AX
	MOVQ b_base+24(FP), CX
	MOVQ a_len+8(FP), DX
	XORQ BX, BX

	// perform vectorized operation for every block of 512 bits
body:
	CMPQ DX, $0x00000008
	JL   tail

	// perform the logical "XOR" operation
	VMOVUPD (CX), Y0
	VMOVUPD 32(CX), Y1
	VPXOR   (AX), Y0, Y0
	VPXOR   32(AX), Y1, Y1
	VMOVUPD Y0, (AX)
	VMOVUPD Y1, 32(AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000040, AX
	ADDQ $0x00000040, CX
	SUBQ $0x00000008, DX
	JMP  body

tail:
	CMPQ DX, $0x00
	JE   done

	// perform the logical "XOR" operation
	MOVQ (CX), BX
	XORQ (AX), BX
	MOVQ BX, (AX)

	// continue the interation by moving read pointers
	ADDQ $0x00000008, AX
	ADDQ $0x00000008, CX
	SUBQ $0x00000001, DX
	JMP  tail

done:
	RET
